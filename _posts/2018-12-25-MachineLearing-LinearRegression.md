---
layout:     post                    # 使用的布局（不需要改）
title:      MachineLearning-LinearRegression     # 标题 
subtitle:   机器学习-线性回归         #副标题
date:       2018-12-25              # 时间
author:     ZYM                     # 作者
header-img: img/first.jpg           #这篇文章标题背景图片
catalog: true                       # 是否归档
---

# MachineLearning-LinearRegression

## 线性回归

用于输出变量和输入变量（特征）成多项式关系  

常用方法是梯度下降和正规方程  

两种方法都用于求代价函数取最优值时的参数（parameter）  

## 梯度下降

梯度下降是最常用的求函数最小值的方法，常用来求代价函数的最小值。  

### 单一特征线性回归

设数据集为 (x, y)  

代价函数为 J(s0, s1) = 1/2m * sum((f(x) - y)^2)  

设预测函数 f(x) = s0 x + s1  

那么迭代方法为(a为学习率，代表了迭代时靠近最终结果的步长，太小迭代次数会太多，太大会得不到精确收敛的解)  

> s0 = s0 -  a * J(s0, s1)对s0的偏导  

s1同理，最终迭代得到收敛的参数值s0, s1  

此时得到映射 f 即为预测函数  

至于此处迭代方式，它可以实现在接近参数最优解时接近的步长减小，从而逐渐接近得到最优参数s0 s1  

同时各个参数接近最优解的步长是各不相同的  

考虑到各个参数对预测结果的影响率，应该对各个特征进行放缩，最好特征都在 -1 到 1 之间  

> 常用 ：x - x平均 / 标准差  

如此保证如房子面积120， 层数2 对结果的贡献相近  

### 多特征线性回归

所谓多特征，是指输入变量有多个，如房子的面积、层数、地址...与价格的关系  

多特征数据进行线性回归与单一特征相似，只是要对多个参数进行迭代，求出代价函数的最小值。  

### 多项式回归

有时特征值与输出不成线性关系，可能为二次或多项式关系，此时修改价值函数，进行多特征回归，依旧可以得到预测函数 f  

## 正规方程

正规方程用于解决数据量较小（小于10000左右）的线性回归问题  

即求解方程 J(s) 对 s 的偏导为0， 此时参数s为所求值  

对一个数据集矩阵X：  
[   x0  x1  x2  ...  xn  y]  
[   ..  ..  ..  ...  ..  .]  
[   ..  ..  ..  ...  ..  .]  
[   ..  ..  ..  ...  ..  .]  

参数向量  
S = (X转置 * X) ^-1  X转置 Y  
